{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bbdbe6-c0ca-4907-a72a-1023dbabac2b",
   "metadata": {},
   "source": [
    "# Create TinyGPT via chatgpt\n",
    "## Prompt\n",
    "\n",
    "Give a python code for create a model called TinyGPT based on Transformer algo that gets trained on a meaningful text  \"It was a bright cold day in April, and the clocks were striking thirteen.\" Also, give code to save the model, load the model and generate a text based on 10 character prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9111f9b-9796-46a4-9c5d-c3ba1f529f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607546f-49dd-4283-a3da-1d27530a1f94",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6cbff7-41e6-46fe-949f-641d3edf6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 32\n",
    "embedding_size = 64\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sample training text\n",
    "text = \"It was a bright cold day in April, and the clocks were striking thirteen.\"\n",
    "\n",
    "# Character-level tokenizer\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = (q @ k.transpose(-2, -1)) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_heads, embedding_size // n_heads)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_size)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embedding_size)\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29353c58-57c2-476f-b460-1a6688500911",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82139b41-224e-4edb-b719-3781e4757c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 3.2469\n",
      "Step 100: Loss 1.1488\n",
      "Step 200: Loss 0.6581\n",
      "Step 300: Loss 0.2267\n",
      "Step 400: Loss 0.5352\n"
     ]
    }
   ],
   "source": [
    "model = TinyGPT().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "def get_batch():\n",
    "    start = random.randint(0, len(data) - block_size - 1)\n",
    "    x = data[start:start+block_size].unsqueeze(0)\n",
    "    y = data[start+1:start+block_size+1].unsqueeze(0)\n",
    "    return x, y\n",
    "\n",
    "# Train for a few steps\n",
    "for step in range(500):\n",
    "    xb, yb = get_batch()\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0115a7-5eae-4e26-b90f-10fff9f321c1",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a43b0e-8164-4bb0-9483-3871925c3825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyGPT(\n",
       "  (token_embedding): Embedding(23, 64)\n",
       "  (pos_embedding): Embedding(32, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SelfAttentionHead(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SelfAttentionHead(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"tinygpt.pth\")\n",
    "\n",
    "# Load model\n",
    "model_loaded = TinyGPT().to(device)\n",
    "model_loaded.load_state_dict(torch.load(\"tinygpt.pth\"))\n",
    "model_loaded.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d837b2-9305-4765-a12f-c9956f0f19d0",
   "metadata": {},
   "source": [
    "# Generate Text from Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e935e4-29e9-4281-aa6c-9655b71f48d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " bright cold day in April, and the the clocks Aprenockiks tre\n"
     ]
    }
   ],
   "source": [
    "# Prompt (should be 10 characters)\n",
    "prompt = \"bright col\"  # Example 10-character prompt\n",
    "input_idx = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate 50 characters\n",
    "output_idx = model_loaded.generate(input_idx, max_new_tokens=50)[0].tolist()\n",
    "print(\"Generated text:\\n\", decode(output_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffd823-2bc4-4be3-999e-8616fb565e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
